
@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	shorttitle = {Grad-{CAM}},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network ({CNN})-based models, making them more transparent and explainable.},
	pages = {336--359},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	urldate = {2023-10-23},
	date = {2020-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {1610.02391.pdf:/home/johan/Downloads/1610.02391.pdf:application/pdf},
}

@misc{alqaraawi_evaluating_2020,
	title = {Evaluating Saliency Map Explanations for Convolutional Neural Networks: A User Study},
	url = {http://arxiv.org/abs/2002.00772},
	shorttitle = {Evaluating Saliency Map Explanations for Convolutional Neural Networks},
	abstract = {Convolutional neural networks ({CNNs}) offer great machine learning performance over a range of applications, but their operation is hard to interpret, even for experts. Various explanation algorithms have been proposed to address this issue, yet limited research effort has been reported concerning their user evaluation. In this paper, we report on an online between-group user study designed to evaluate the performance of "saliency maps" - a popular explanation algorithm for image classification applications of {CNNs}. Our results indicate that saliency maps produced by the {LRP} algorithm helped participants to learn about some specific image features the system is sensitive to. However, the maps seem to provide very limited help for participants to anticipate the network's output for new images. Drawing on our findings, we highlight implications for design and further research on explainable {AI}. In particular, we argue the {HCI} and {AI} communities should look beyond instance-level explanations.},
	number = {{arXiv}:2002.00772},
	publisher = {{arXiv}},
	author = {Alqaraawi, Ahmed and Schuessler, Martin and Weiß, Philipp and Costanza, Enrico and Berthouze, Nadia},
	urldate = {2023-10-23},
	date = {2020-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.00772 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Evaluating Saliency Map Explanations for Convolutional Neural Networks A User Study - 2002.00772.pdf:/home/johan/Downloads/Evaluating Saliency Map Explanations for Convolutional Neural Networks A User Study - 2002.00772.pdf:application/pdf},
}
