%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   FUNDAMENTAL ELEMENTS OF MACHINE LEARNING
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
In this chapter I will give a brief introduction into machine learning. This includes a mathematical description of some fundamental concepts common across numerous machine learning models. The more advanced models will be dealt with at a later stage. If not otherwise stated, the following chapter is based on ~\cite{Goodfellow-et-al-2016} and ~\cite{hastie2009elements}. 

\section{Linear Algebra}
maybe

\section{Probability and Information Theory}
maybe

\section{Basic Machine Learning}
\TODO{Fill more here}

    \subsection{Optimisation and Generalisation}
        \paragraph{Optimisation}
        Optimsation problems are problems in which we want to minimise some error, given some data. In other words, we want to optimise an algorithm or model given a specific dataset. We care about the error of our model for that specific dataset only.

        \paragraph{Generalisation}
        The concept of \textit{generalisation} is what makes a machine learning model different from an optimisation model. We still \textit{train} the machine learning model on some specific data, but we measure how good the model is based on how it performs on a different set of data, which it was not trained on. I.e. we need a generalised model, which is not restricted to the data it was trained on.

    \subsection{Data and Fitting}
        The key ingredient to any machine learning algorithm is the data fed into it. 

    \subsection{Estimators, Bias, Variance and Error}

        \paragraph{Estimators} 
        Based on the assumption that there exists some true parameter(s) $\svec{\theta}$ which remain unknown,\footnote{This is the frequentist perspective of statistics} we are able to make predictions and estimations of such parameter(s). Let's say we have $m$ independent and identically distributed (i.i.d.) random variables $\{\vec{x}_1, \vec{x}_2, \dots, \vec{x}_m\}$ drawn from the same probability distribution $p(\vec{x})$. An \textit{estimator} of the true values $\svec{\theta}$ is any function of the data such that $\svec{\hat{\theta}}_m = g(\vec{x}_1,\dots,\vec{x}_m)$, where $\svec{\hat{\theta}}$ is the estimate of $\svec{\theta}$. This is known as point estimation, as we are estimating a single value. This definition does not pose any restrictions on the function $g$. However, a good estimator would yield an estimate $\svec{\hat{\theta}}_m$ that is close to the true value $\svec{\theta}$. 
        \paragraph{Function estimators}
        Say we want to predict a variable $\vec{y}$ given some vector $\vec{x}$. We assume the true variable $\vec{y}$ is given by some function approxiamation $f(\vec{x})$ plus some error $\svec{\epsilon}$: $\vec{y} = f(\vec{x}) + \svec{\epsilon}$. The aim is then to estimate the function $f$ with the estimator $\hat{f}$. If we then realise that $\hat{f}$ is really just a point estimator in function space, the two above concepts are equivalent.

        \paragraph{Bias}
        The bias of the estimator $\svec{\hat{\theta}}_m$ is defined as the difference between the expected value of the estimator and the true value of the parameter: $\text{bias}(\svec{\hat{\theta}}_m) = \EE{\estm} - \svec{\theta}$. An unbiased estimator has zero bias, i.e. $\EE{\estm} = \svec{\theta}$. An estimator is asymptotically unbiased if its bias approaches zero as the number of data points $m$ approaches infinity, i.e. $\lim_{m\to\infty} \EE{\estm} = \svec{\theta}$.

        \paragraph{Variance}


        \paragraph{Standard Error}

        \paragraph{Mean Squared Error}


    \subsection{Maximum Likelihood Estimation}
    \subsection{Bayesian Statistics}
    \subsection{Supervised Learning}
    \subsection{Unsupervised Learning}
