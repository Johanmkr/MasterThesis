%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   NEURAL NETWORKS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Forward pass - Prediction}
    \subsection{Computational Graphs}
    In order to understand how information flows through a neural network we must understand computational graphs. These are collection of \textit{nodes} and \textit{edged}, where each node represents an \textit{operation} and each edge represents the numerical values as they flow through the network. Nodes may be simple functions such as addition or multiplication but also take more complex. In the latter case, they can be further divided into their respective component, so that the most fundamental computational graph only consists fundamental operations. 

    For example, we consider the functions $\yv = g(\xv)$ and $\zv = f(\yv)$ such that $\zv = f(g(\xv))$. This can be represented as a computational graph as shown in \cref{fig:ML:NN:comp_graph_example}. The computational graph is read from left to right, where the input $\xv$ is fed into the function $g$ which outputs $\yv$. This $\yv$ is then fed into the function $f$ which outputs $\zv$. The computational graph is a visual representation of the function $f(g(\xv))$.

    \begin{figure}[h!]
        \centering
        \input{TikZ/ML/computational_graph_example.tex}
        \caption{Computational graph of a function $\vec{z} = f(g(\xv))$.}
        \label{fig:ML:NN:comp_graph_example}
    \end{figure}




    

    \subsection{Architecture}
        The simplest architecture of a neural network is the fully connected feed forward (FCNN), which consist of $L$ layers in total, the first being an input layer and the remaining $L-1$ layers are called \textit{hidden layers}, $\vec{h}$. Each hidden layer has an \textit{activation} $\vec{a}$, used as an input to an \textit{activation function}, $g$. Mathematically we may write this architecture as:
        \begin{equation}
            \begin{split}
                \vec{h}^0 &= \vec{x}^{(i)} \\
                \vec{h}^1 &= g_1(\vec{a}^1) \\
                \vec{h}^2 &= g_2(\vec{a}^2) \\
                \vdots & \\
                \vec{h}^{L} &= g_L(\vec{a}^L),
            \end{split}
        \end{equation}
        where $\vec{h}^L$ and $g_L$ are the output layer and output function respectively. The parameter $L$ governs the depth of the neural network. The result of the output layer is simply called the \textit{output} or \textit{predictor}, and typically denoted as $\pred = \vec{h}^L$. When training, we will have a true value, or label, denoted $\yv$. The difference between the true value and the predicted value is called the \textit{residual} or \textit{error}, denoted $\tilde{\yv} = \yv - \pred$. The goal of training is to minimise the error, or equivalently, maximise the likelihood of the true value.
    \subsection{Activation}
        The activation $\vec{a}^l$ of a layer $l$ is an affine transformation of the output of the previous layer, $\vec{h}^{l-1}$. The intercept of this affine transformation is known as the bias $\vec{b}^l$, \footnote{Must not be confused with the statistical bias of an estimator.} typically used to ensure that no activation becomes zero. The activation takes the form:
        \begin{equation}
            \vec{a}^l = (\vec{W}^{l-1\to l})^\mathrm{T}\vec{h}^{l-1} + \vec{b}^l,
        \end{equation}
        where $\vec{W}^{l-1\to l} \in \mathbb{R}^{\mathrm{dim}_{l-1}\cross\mathrm{dim}_l}$ is the matrix of weights describing the mapping from layer $l-1$ to layer $l$. Each layer $l$ has dimension (or neurons) $\mathrm{dim}_l$ which governs the width of each layer. 

    \subsection{Activation functions}
        The activation functions, $g_l$, are what determines the output of each layer $l$. They can be broadly classifies into two types: \textit{saturating activation functions} and \textit{non-saturating activation function}. For saturating activations, $\lim_{\abs{\vec{x}}\to\infty}\abs{\nabla g(\vec{x})} = 0$. 
        \paragraph{Linear}
            The linear activation is as simple as it gets:
            \begin{equation}
                g(\vec{x}) = a\vec{x} + b \quad \mathrm{and} \quad g'(\vec{x}) = a.
            \end{equation}
            This activation is non-saturating. There is however a problem with using linear activations, \TODO{Write about UNIVERSAL APPROXIMATION THEOREM, here or elsewhere}

        \paragraph{Sigmoid}
            The sigmoid function, or the logistic function is defined as: 
            \begin{equation}
                g(\vec{x}) = \sigma(\vec{x}) = \frac{1}{1+\expe{-\vec{x}}} \quad \mathrm{and} \quad g'(\vec{x}) = g(\vec{x})(1-g(\vec{x}))
            \end{equation}


        \paragraph{Hyperbolic tangent}
            The hyperbolic tangent is defined as:
            \begin{equation}
                g(\vec{x}) = \frac{\expe{\vec{x}}-\expe{-\vec{x}}}{\expe{\vec{x}}+\expe{-\vec{x}}} \quad \mathrm{and} \quad g'(\xv)=1-g(\xv)^2
            \end{equation}

        \paragraph{Rectified linear unit}
            The rectified linear unit, ReLU, is defined as:
            \begin{equation}
                g(\xv) = 
                \begin{cases}
                    0, \quad & \xv < 0 \\
                    \xv, \quad & \xv \geq 0
                \end{cases}
                \quad \mathrm{and} \quad
                g'(\xv) = 
                \begin{cases}
                    0, \quad & \xv < 0 \\
                    1, \quad & \xv \geq 0.
                \end{cases}
            \end{equation}

        \paragraph{(Scaled) Exponential linear unit}
            The exponential linear unit, (S)ELU, is defined as:
            \begin{equation}
                g(\xv) = \lambda
                \begin{cases}
                    \alpha(\expe{\xv}-1), \quad & \xv < 0 \\
                    \xv, \quad & \xv \geq 0
                \end{cases}
                \quad \mathrm{and} \quad
                g'(\xv) = \lambda
                \begin{cases}
                    \alpha\expe{\xv}, \quad & \xv < 0 \\
                    1, \quad & \xv \geq 0.
                \end{cases}
            \end{equation}
            This function requires two hyperparameters $\alpha$ and $\lambda$.

        \paragraph{Parametric rectified linear unit}
            The parametric rectified linear unit, PReLU, is defined as:
            \begin{equation}
                g(\xv) = 
                \begin{cases}
                    \alpha \xv, \quad & \xv<0 \\
                    \xv, \quad & \xv\geq 0
                \end{cases}
                \quad \mathrm{and} \quad
                q'(\xv) = 
                \begin{cases}
                    \alpha, \quad & \xv<0 \\
                    1, \quad & \xv\geq 0.
                \end{cases}
            \end{equation}
            If $\alpha=0.01$ this function is also known as the leaky rectified unit, or Leaky ReLU. 

        \paragraph{Softmax}
            The softmax activation function is defined as:
            \begin{equation}
                g(\xv) = \frac{\expe{\xv}}{\sum_{i=1}^n\expe{x_i}} \quad \mathrm{and} \quad g'(\xv) = g(\xv)(1-g(\xv)).
            \end{equation}
            This activation function is typically used for the output layer of a neural network, where the output is a probability distribution over $n$ classes, i.e. multiclass classification problems. 
        \begin{figure}[h!]
            \centering
            \input{TikZ/ML/computational_graph_hidden_layer.tex}
            \caption{Computational graph of a hidden layer of a neural network.}
            \label{fig:ML:NN:comp_graph_hl}
        \end{figure}
    \subsection{Loss functions}
        \TODO{write about loss functions}
        \paragraph{MSE}
            \begin{equation}
                \mathcal{L}_\mathrm{MSE}(\yv, \pred) = \frac{1}{n}\sum_{i=1}^n(y_i-\hat{y}_i)^2 = \frac{1}{n}\tilde{\yv}^\mathtt{T}\tilde{\yv},
            \end{equation}
            where $\tilde{\yv} \equiv \yv-\pred$

        \paragraph{Huber}
            \begin{equation}
                \mathcal{L}_\delta(\yv, \pred) = 
                \begin{cases}
                    \frac{1}{2}\tilde{\yv}^\mathtt{T}\tilde{\yv}, \quad & \abs{\tilde{\yv}}\leq\delta \\
                    \delta(\abs{\tilde{\yv}} - \delta/2), \quad & \mathrm{otherwise}
                \end{cases}
            \end{equation}
            \TODO{Check if above should perhaps be element wise, use sigma notationa instead?}


        \paragraph{Cross entropy}

        \paragraph{Binary cross entropy}

        \paragraph{Categorical cross entropy}

\section{Backpropagation - Training}
    \subsection{Chain rule}
        \paragraph{Basics}
        Lets say we have a function $f(x)$ that is composed of two functions $g(x)$ and $h(x)$, such that $f(x) = g(h(x))$. The chain rule states that the derivative of $f(x)$ with respect to $x$ is the product of the derivative of $g(x)$ with respect to $h(x)$ and the derivative of $h(x)$ with respect to $x$:
        \begin{equation}
            \frac{df}{dx} = \frac{dg}{dh} \frac{dh}{dx}.
        \end{equation}
        This can be generalised into the vector case where $f(\vec{x}) = g(h(\vec{x}))$ and $\vec{x} \in \mathbb{R}^n$ and \TODO{write more ahaha}
    \subsection{Gradient descent}
    \subsection{Optimizers}
    \subsection{Regularization} % May belongs to forwards pass
    % \TODO{Add dropout, batch normalization, weight decay, early stopping, data augmentation, etc.}




