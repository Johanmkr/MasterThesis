%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   BINARY CLASSIFIER
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\bcol{Primer on Information Theory}}
    \CITE{Goodfellow and colah information theory blogpost}
    \subsection{\bcol{Elements of Surprise}}
        In order to quantify the amount of information in an event, we need to define the notion of \textit{surprise}. This is essentially a measure of how surprised we would be if an event occured. Surely, if an event is very probable we would not be very surprised if it occurred, whereas if it was highly unlikely we would be very surprised. We would therefore expect the surprise of an even $x$ to be inversely proportional to its probability $P(x)$, i.e. $\mathrm{surprise} \propto \frac{1}{P(x)}$. However, if the probability of an event is 1, the surprise would also be one, but if know something is going to happen we would not be very surprised when it actually does happen. Likewise if  $P(x)=0$, the surprise would be infinite (or undefined) which is not very useful. Therefore, we define the surprise, or rather the \textit{self-information} $I(x)$ of an event $x$ as the logarithm of the reciprocal of its probability:
        \begin{equation}\label{eq:ML:binary_classifier:information_theory:self_information}
            I(x) = \log \frac{1}{P(x)} = -\log P(x).
        \end{equation}
        The base of the logarithm determines the unit of $I$. For example, if we use the natural logarithm with base $e$, then $I$ is given in units of \textit{nats}, which is information you receive/obtain by considering an event with probability $e^{-1} \approx 0.368$. If we instead use the base 2, then $I$ is given in units of \textit{bits}, which is information you receive/obtain by considering an even with probability $2^{-1} = 0.5$

        We take this one step further and define the \textit{entropy} $H(P)$ of a random variable $x$ as the expected self-information. This is the average amount of information we receive when we observe the outcome of a random variable. The entropy\footnote{This is called the \textit{Shannon entropy} when $x$ is discrete and \textit{differential entropy} when $x$ is continuous.} is therefore given by:
        \begin{equation}\label{eq:ML:binary_classifier:information_theory:entropy}
            H(P) = \mathbb{E}_{x \sim P}[I(x)] = -\mathbb{E}_{x \sim P}[\log P(x)] = -\sum_{x} P(x) \log P(x).
        \end{equation}
    
    \subsection{\bcol{Cross-Entropy}}
        Why is entropy important? Because it allows us to define the \textit{cross-entropy} $H(P, Q)$ between two probability distributions $P$ and $Q$ as the average amount of information needed to identify an event drawn from the probability distribution $P$, if we use a coding scheme optimized for a probability distribution $Q$:
        \begin{equation}
            H(P,Q) = -\mathbb{E}_{x \sim P}[\log Q(x)] = -\sum_{x} P(x) \log Q(x).
        \end{equation}
        $P$ and $Q$ are two \textit{different} distributions and in general $H(P,Q) \neq H(Q,P)$. The cross-entropy is therefore not a true metric, but it is still a useful quantity. The cross-entropy is minimized when $P=Q$, in which case $H(P,Q) = H(P)$. The cross-entropy is therefore some measure of how different two probability distributions are.
    
    \subsection{\bcol{Kullback-Leibler Divergence}}
        The \textit{Kullback-Leibler divergence} (KL divergence) $D_{KL}(P||Q)$ is a measure of the difference between the cross-entropy $H(P,Q)$ and the entropy $H(P)$:
        \begin{equation}\label{eq:ML:binary_classifier:information_theory:KL_divergence}
            D_{KL}(P||Q) = H(P,Q) - H(P) = \mathbb{E}_{x \sim P}[\log P(x) - \log Q(x)] = \mathbb{E}_{x \sim P}\left[\log \frac{P(x)}{Q(x)}\right].
        \end{equation}
        We can see that the KL divergence is not symmetric, i.e. $D_{KL}(P||Q) \neq D_{KL}(Q||P)$. The KL divergence is also minimized when $P=Q$, in which case $D_{KL}(P||Q) = 0$. It is therefore also a measure of the difference between two probability distributions. We likewise write the cross entropy as the sum of the entropy and the KL divergence:
        \begin{equation}\label{eq:ML:binary_classifier:information_theory:cross_entropy_as_sum_of_entropy_and_KL_divergence}
            H(P,Q) = H(P) + D_{KL}(P||Q),
        \end{equation}
        where we clearly see that minimizing the cross-entropy and the KL-divergence are equivalent, given that $H(P)$ is constant, which it will be if $P$ is a fixed, known probability distribution.


\section{\bcol{Input-Output}}
        We will have a known dataset, with known classes. Hence, the target values of our model will be known, and we denote them $\yv$, where $y_i$ is the probability that sample $i$ is simulated with GR. The output of our model will be the predicted target values, and we denote them $\hat{\yv}$, where $\hat{y}_i$ is the predicted probability that sample $i$ is simulated with GR. The output of our model is therefore a probability distribution over the two classes. \TODO{Correct notation for input image/field}

    \subsection{\bcol{Input Distribution}}
        The distribution of the input labels is known, since we know the class of each sample in the dataset. We can write $y_i\sim Y(y)$, where $Y(y)$ is the probability distribution of the input labels. Since $y_i$ can only take the values 0 or 1, $Y(y)$ must be a Bernoulli distribution, with $\phi=0.5$, i.e. $Y(y) = \mathrm{Bernoulli}(y; \phi=0.5)$. This means that the probability of a sample being simulated with GR is 0.5, and the probability of a sample being simulated with NG is also 0.5. This is because the dataset is balanced, i.e. there are as many samples simulated with GR as there are samples simulated with NG. 

    \subsection{\bcol{Output Distribution}}
        The model will yield an output $\hat{y}_i$, which is the predicted probability that sample $i$ is simulated with GR. We can write $\hat{y}_i \sim \hat{Y}(\hat{y})$, where $\hat{Y}(\hat{y})$ is the probability distribution of the output labels. Since $\hat{y}_i$ can only take the values 0 or 1\Question{correct}, $\hat{Y}(\hat{y})$ must also be a Bernoulli distribution. The model thus seek to approximate the distribution of the known targets. 

    \subsection{\bcol{Comparison of Distributions}}
        Given a known target distribution $Y$ and a predicted target distribution $\hat{Y}$, we can compare the two distributions through either the KL-divergence or the cross-entropy. For computational reasons, we will use the cross-entropy. The cross-entropy between the two distributions will be:
        \begin{equation}
            H(Y, \hat{Y}) = -\sum_{n} Y(\yv = n) \log \hat{Y}(\hat{\yv}=n),
        \end{equation}
        where $n\in\{\mathrm{GR, NG}\}$. For a given sample $i$, the probability that it is simulated with GR is $y_i$, and the probability that it is simulated with NG is $1-y_i$. The cross-entropy for this sample is therefore:
        \begin{equation}
            H(y_i, \hat{y}_i) = -y_i \log \hat{y}_i - (1-y_i) \log (1-\hat{y}_i) = \mathcal{L}_\mathrm{BCE}(y_i, \hat{y}_i),
        \end{equation}
        which is what we earlier defined as the binary cross-entropy loss function in equation \cref{eq:ML:NN:loss:binary_cross_entropy}.

\section{\rcol{Assessing a Binary Classifier}}
    \subsection{\rcol{Loss}}
        \begin{equation}
            \mathrm{Loss} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(y_i, \hat{y}_i),
        \end{equation}
        where $N$ is the number of samples, $y_i$ is the true label of sample $i$ and $\hat{y}_i$ is the predicted label of sample $i$. $\mathcal{L}$ is the loss function, which is a function of the true label and the predicted label. The loss function is a measure of how well the classifier performs. The loss function is minimized when the classifier performs optimally. The loss function is therefore a measure of the performance of the classifier. The loss function is also called the \textit{cost function} or the \textit{objective function}.
    \subsection{\ycol{Confusion Matrix}}
        \begin{table}[H]
            \centering
            \begin{tabular}{c|c|c|}
                \cline{2-3}
                & \multicolumn{2}{c|}{\textbf{Predicted}} \\ \cline{2-3} 
                & \textbf{Positive} & \textbf{Negative} \\ \hline
                \multicolumn{1}{|c|}{\textbf{Actual Positive}} & True Positive (TP) & False Negative (FN) \\ \hline
                \multicolumn{1}{|c|}{\textbf{Actual Negative}} & False Positive (FP) & True Negative (TN) \\ \hline
            \end{tabular}
            \caption{Confusion matrix for a binary classifier.}
            \label{tab:ML:binary_classifier:confusion_matrix}
        \end{table}
        \begin{itemize}
            \item \textbf{True Positive (TP)}: The number of positive samples that were correctly classified as positive.
            \item \textbf{False Negative (FN)}: The number of positive samples that were incorrectly classified as negative.
            \item \textbf{False Positive (FP)}: The number of negative samples that were incorrectly classified as positive.
            \item \textbf{True Negative (TN)}: The number of negative samples that were correctly classified as negative.
        \end{itemize}
    \subsection{\ycol{Performance Metrics}}
        \begin{equation}
            \mathrm{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
        \end{equation}
        \begin{equation}
            \mathrm{Precision} = \frac{TP}{TP + FP}
        \end{equation}
        \begin{equation}
            \mathrm{Recall} = \frac{TP}{TP + FN}
        \end{equation}
        \begin{equation}
            \mathrm{F1} = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
        \end{equation}
        \begin{equation}
            \mathrm{F1} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
        \end{equation}