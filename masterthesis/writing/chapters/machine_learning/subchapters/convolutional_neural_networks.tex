%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   CONVLUTIONAL NEURAL NETWORKS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Convolution}
    \subsection{Basic definitions}
        Mathematically, convolution is given as:
        \begin{equation}\label{eq:ML:CNN:convolution:basic_defintion}
            s(t) = (x * w)(t) = \int x(a)w(t-a)\d a.
        \end{equation}
        This definition is purely mathematical. For machine learning purposes we refer to the function $x(a)$ as the \textit{input} and $w(t-a)$ as the \textit{kernel}. The output of the convolution is often called the \textit{feature map}. This is easily generalized to higher dimensions. For example, in two dimensions the discrete convolution between an input image $I$ and a kernel $K$ is given as:
        \begin{equation}\label{eq:ML:CNN:convolution:discrete_convolution}
            S(i,j) = (I * K)(i,j) = \sum_m\sum_n I(m,n)K(i-m,j-n).
        \end{equation}
        If we choose to $\textit{flip the kernel}$ relative to the input $I$, the convolution becomes commutative:
        \begin{equation}\label{eq:ML:CNN:convolution:discrete_convolution_commutative}
            S(i,j) = (K * I)(i,j) = \sum_m\sum_n I(i-m,j-n)K(m,n).
        \end{equation}
        The latter is preferred in a machine learning context as there is less variation in the values of $m$ and $n$ because the size of the kernel is often much smaller than the size of the input. However, there is yet another quantity that is often used in machine learning, namely the \textit{cross-correlation}, which is more intuitive in some sense, because the kernel is not flipped. The cross-correlation is given as:
        \begin{equation}\label{eq:ML:CNN:convolution:cross_correlation}
            S(i,j) = (K \star I)(i,j) = \sum_m\sum_n I(i+m,j+n)K(m,n).
        \end{equation}
        Most convolutional layers in machine learning use the cross-correlation. However, it is often referred to as just a convolution without the kernel flip, so this is the convention used further. An example of such a convolution (kernel not flipped) is illustrated in figure \ref{fig:ML:CNN:convolution:convolution_operation_map_example}.

        \begin{figure}[h!]
            \centering
            \input{TikZ/ML/convolution_operation_map_example.tex}
            \caption{\TODO{fix this figure to be correct w.r.t. cross-correlation/convolution} Example of a cross-correlation operation, which we will also call a convolution. The input image is given by the matrix $\mathbf I$ and the kernel is given by the matrix $\mathbf K$. The output is given by the matrix $\mathbf{K \star I}$. The dashed lines indicate which elements of the input and kernel are multiplied together to produce the output. The figure is taken from TikZ.net \TODO{fix this citation}}
            \label{fig:ML:CNN:convolution:convolution_operation_map_example}
        \end{figure}
        Whether the kernel is flipped does not play a huge role in a convolutional neural networks, because the kernel is learned by the network. However, it is important to be aware of the difference between the two definitions, because it can lead to confusion when interpreting both the output of the network and the convolutional layers themselves. 

    \subsection{Properties}
        \paragraph{Feature detection}
            Different kernel has the ability to detect different features in the input. For example, different kernels can detect vertical edges, horizontal edges, corners or even more complex features. These kernels may be manually defined, however this defies the basic concepts of machine learning. These kernels are instead learned by the network, analogous to the weight matrices $\vec{W}$ in a fully connected network. Hence, the output of a convolutional operation is a feature map. 

        \paragraph{Sparse interactions}
            The kernels $\vec{K}$ are usually a lot smaller than the input images $\vec{I}$. This means that each element of the kernel is used multiple times when producing the output feature map, which again allows for fewer parameters stored and faster computations. 

        \paragraph{Parameter sharing}
            Exactly what it sounds like, the same parameters are re-used to produce more than one output. The learnable weights are the same for each element of the feature map. This is also a way to reduce the number of parameters stored and the number of computations needed.

        \paragraph{Equivariance}
            Convolutional layers are equivariant to translation. This means that if the input is translated, the output is translated by the same amount. This is a very useful property when dealing with images, because the location of a feature is not important. For example, if we want to detect an edge in an image, it does not matter where in the image the edge is located.
        All of the above properties are related to each other, but their combinations makes convolutional layers in a network very powerful.
\section{New Layers}
    \subsection{Convolutional layers}
        A convolutional layer is just a layer in the neural network that uses the convolution (or rather the cross-correlation) operation rather than just plain matrix multiplication. Both the input and output may consist of several planes or channels. For example, an input image may have three channels, one for each color (RGB). The output of a convolutional layer may also have several channels. This is useful because it allows the network to learn different features in parallel. For example, one channel may learn to detect vertical edges, while another channel may learn to detect horizontal edges. The output of the convolutional layer is then a stack of feature maps, one for each channel. Let's consider convolutions in 2 dimensions, where we have $C_\mathrm{in}$ input channels and $C_\mathrm{out}$ output channels. The convolutional layer is then defined as:
        \begin{equation}
            \vec{S}(N_i, C_{\mathrm{out}_j}) = \mathrm{bias}(C_{\mathrm{out}_j}) + \sum_{k=1}^{C_\mathrm{in}} \vec{K}(C_{\mathrm{out}_j}, k) \star \vec{I}(N_i, k),
        \end{equation}
        where $N_i$ is some batch of the dataset. In the case where we omit the bias for the convolutional layers and only have one input channel, this reduces to the simpler form: 
        \begin{equation}
            \vec{S}(N_i, C_{\mathrm{out}_j}) = \vec{K}(C_{\mathrm{out}_j}) \star \vec{I}(N_i).
        \end{equation} 
        The above equation uses $C_{\mathrm{out}}$ different kernels to produce the same number of feature maps. 

        \TODO{Figures to show kernel size, padding, stride and dilation}

        \paragraph{Kernel size}
            The kernel size is usually chosen to be odd, because this allows for a central pixel. This is not necessary, but it is a common choice. The kernel size is usually chosen to be small, because this allows for more parameters to be stored in the network. However, the kernel size is usually chosen to be large enough to capture the features we want to detect.

        \paragraph{Padding}
            The padding is used to control the size of the output feature maps. If we do not use any padding, the output feature maps will be smaller than the input feature maps. This is because the kernel cannot be centered on the edges of the input feature maps. The padding is usually chosen to be zero padding, which means that the input feature maps are padded with zeros. This is the simplest choice, but there are other choices as well. For example, we may choose to pad the input feature maps with the edge values. This is called \textit{edge padding}. Another choice is to pad the input feature maps with the reflection of the edge values. This is called \textit{reflection padding}. The padding is usually chosen to be symmetric, which means that the same amount of padding is added to each side of the input feature maps. However, this is not necessary. 

        \paragraph{Stride}
            The stride is used to control the size of the output feature maps. If we do not use any stride, the output feature maps will be smaller than the input feature maps. This is because the kernel cannot be centered on the edges of the input feature maps. The stride is usually chosen to be one, which means that the kernel is moved one pixel at a time. However, the stride can be chosen to be larger than one, which means that the kernel is moved several pixels at a time. The stride is usually chosen to be the same in both the horizontal and vertical directions, but this is not necessary.

        \paragraph{Dilation}
            The dilation is used to control the size of the output feature maps. If we do not use any dilation, the output feature maps will be smaller than the input feature maps. This is because the kernel cannot be centered on the edges of the input feature maps. The dilation is usually chosen to be one, which means that the kernel is moved one pixel at a time. However, the dilation can be chosen to be larger than one, which means that the kernel is moved several pixels at a time. The dilation is usually chosen to be the same in both the horizontal and vertical directions, but this is not necessary.

        All of the above would define how convolution should take place, and also control the dimensions of the resulting feature map. If we, along an axis, have an input dimension of $D_\mathrm{in}$, a kernel size of $K$, a padding of $P$, a stride of $S$ and a dilation of $D$, the output dimension, $D_\mathrm{out}$, is given as:
        \begin{equation}
            D_\mathrm{out} = \left\lfloor {\frac{D_\mathrm{in} + 2P-D(K-1)-1}{S}+1} \right\rfloor
        \end{equation}
    \subsection{Pooling layers}
        Pooling is a form of dimensional reduction. It is used to reduce the size of the feature maps. This is useful because it reduces the number of parameters stored and the number of computations needed. Pooling is usually done in the spatial dimensions, but it can also be done in the channel dimension. Pooling often consists of replacing the output of a small region of the feature map with some summary statistics for that region. Examples of summary statistics used may be max pooling, average pooling or $L^2$ pooling. Max pooling is the most common choice. A consequence of pooling is invariance to local translation. Also note that pooling may not have to bee a separate operation. Similar effects of dimension reduction and translation invariance may equally be obtained by choosing kernel size, padding, stride and dilation such that the dimension of the feature map is reduced. 
